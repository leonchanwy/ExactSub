import streamlit as st
import requests
import openai
import opencc
import json
import logging
import io
import time
import hashlib
from pathlib import Path
import pandas as pd

# --- 0. Logging è¨­å®š ---
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.propagate = False

def get_log_stream():
    """å–å¾—æˆ–å»ºç«‹ session å°ˆå±¬çš„ log stream"""
    if 'log_stream' not in st.session_state:
        st.session_state.log_stream = io.StringIO()
    return st.session_state.log_stream

def setup_logger():
    """è¨­å®š loggerï¼Œç¢ºä¿æ¯å€‹ session æœ‰ç¨ç«‹çš„ handler"""
    log_stream = get_log_stream()

    # ç§»é™¤èˆŠçš„ handlers é¿å…é‡è¤‡
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # å»ºç«‹æ–°çš„ handlers
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

    stream_handler = logging.StreamHandler(log_stream)
    stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

    logger.addHandler(console_handler)
    logger.addHandler(stream_handler)

SUPPORTED_AUDIO_TYPES = ["mp3", "wav", "m4a", "flac", "ogg"]

MIME_TYPE_MAP = {
    'mp3': 'audio/mpeg',
    'wav': 'audio/wav',
    'm4a': 'audio/mp4',
    'flac': 'audio/flac',
    'ogg': 'audio/ogg',
}

UPLOAD_MAX_SIZE_MB = 500

def get_mime_type(filename):
    """æ ¹æ“šæª”æ¡ˆå‰¯æª”åå–å¾—å°æ‡‰çš„ MIME type"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    return MIME_TYPE_MAP.get(ext, 'audio/mpeg')

# --- API é©—è­‰èˆ‡é¡åº¦æª¢æŸ¥ ---

def safe_int(value, default=0):
    """å°‡æ•¸å€¼å®‰å…¨è½‰ç‚º intï¼Œå¤±æ•—å‰‡å›å‚³é è¨­å€¼"""
    try:
        return int(value)
    except (TypeError, ValueError):
        return default

def validate_elevenlabs_key(api_key):
    """é©—è­‰ ElevenLabs API Key ä¸¦å–å¾—å¸³æˆ¶è³‡è¨Š"""
    try:
        url = "https://api.elevenlabs.io/v1/user/subscription"
        headers = {"xi-api-key": api_key}
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code == 401:
            return {"valid": False, "error": "API Key ç„¡æ•ˆ"}
        elif response.status_code != 200:
            return {"valid": False, "error": f"API éŒ¯èª¤: {response.status_code}"}

        data = response.json()
        character_count = safe_int(data.get("character_count"))
        character_limit = safe_int(data.get("character_limit"))

        return {
            "valid": True,
            "tier": data.get("tier", "unknown"),
            "character_count": character_count,
            "character_limit": character_limit,
            "remaining_characters": max(0, character_limit - character_count),
        }
    except requests.RequestException as e:
        return {"valid": False, "error": f"é€£ç·šéŒ¯èª¤: {str(e)}"}

def validate_openai_key(api_key):
    """é©—è­‰ OpenAI API Key"""
    try:
        client = openai.OpenAI(api_key=api_key)
        # ä½¿ç”¨ç°¡å–®çš„ models list ä¾†é©—è­‰ key
        models = client.models.list()
        return {"valid": True, "models_count": len(list(models))}
    except openai.AuthenticationError:
        return {"valid": False, "error": "API Key ç„¡æ•ˆ"}
    except openai.APIError as e:
        return {"valid": False, "error": f"API éŒ¯èª¤: {str(e)}"}
    except Exception as e:
        return {"valid": False, "error": f"éŒ¯èª¤: {str(e)}"}

# --- è²»ç”¨è¨ˆç®— ---

# æ¨¡å‹å®šåƒ¹ (æ¯ 1M tokensï¼ŒUSD)
# è‹¥æœªçŸ¥ï¼Œè¨­ç‚º Noneï¼Œé¿å…é¡¯ç¤ºéŒ¯èª¤æˆæœ¬ä¼°ç®—
MODEL_PRICING = {
    "gpt-5.2": {"input": 1.75, "output": 14.00},
    "gpt-5-mini": {"input": 0.25, "output": 2.00},
    "gpt-4.1": {"input": 2.00, "output": 8.00},
    "gpt-4.1-mini": None,
}

# ElevenLabs Scribe å®šåƒ¹ ($0.48/hour = $0.008/åˆ†é˜)
ELEVENLABS_SCRIBE_PRICE_PER_MINUTE = 0.008

def init_cost_tracker():
    """åˆå§‹åŒ–è²»ç”¨è¿½è¹¤å™¨"""
    if 'cost_tracker' not in st.session_state:
        st.session_state.cost_tracker = {
            "elevenlabs_minutes": 0.0,
            "elevenlabs_cost": 0.0,
            "openai_input_tokens": 0,
            "openai_output_tokens": 0,
            "openai_cost": 0.0,
            "openai_pricing_available": True,
            "openai_pricing_model": "",
            "total_cost": 0.0,
        }

def reset_cost_tracker():
    """é‡ç½®è²»ç”¨è¿½è¹¤å™¨"""
    st.session_state.cost_tracker = {
        "elevenlabs_minutes": 0.0,
        "elevenlabs_cost": 0.0,
        "openai_input_tokens": 0,
        "openai_output_tokens": 0,
        "openai_cost": 0.0,
        "openai_pricing_available": True,
        "openai_pricing_model": "",
        "total_cost": 0.0,
    }

def track_elevenlabs_cost(audio_duration_seconds):
    """è¿½è¹¤ ElevenLabs è²»ç”¨"""
    minutes = audio_duration_seconds / 60.0
    cost = minutes * ELEVENLABS_SCRIBE_PRICE_PER_MINUTE
    st.session_state.cost_tracker["elevenlabs_minutes"] += minutes
    st.session_state.cost_tracker["elevenlabs_cost"] += cost
    st.session_state.cost_tracker["total_cost"] += cost
    return cost

def track_openai_cost(model, input_tokens, output_tokens):
    """è¿½è¹¤ OpenAI è²»ç”¨"""
    pricing = MODEL_PRICING.get(model)

    st.session_state.cost_tracker["openai_input_tokens"] += input_tokens
    st.session_state.cost_tracker["openai_output_tokens"] += output_tokens

    if not pricing:
        st.session_state.cost_tracker["openai_pricing_available"] = False
        st.session_state.cost_tracker["openai_pricing_model"] = model
        return 0.0

    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    total_cost = input_cost + output_cost

    st.session_state.cost_tracker["openai_cost"] += total_cost
    st.session_state.cost_tracker["total_cost"] += total_cost
    return total_cost

def get_cost_summary():
    """å–å¾—è²»ç”¨æ‘˜è¦"""
    tracker = st.session_state.cost_tracker
    return {
        "elevenlabs": {
            "minutes": tracker["elevenlabs_minutes"],
            "cost": tracker["elevenlabs_cost"],
        },
        "openai": {
            "input_tokens": tracker["openai_input_tokens"],
            "output_tokens": tracker["openai_output_tokens"],
            "cost": tracker["openai_cost"],
            "pricing_available": tracker["openai_pricing_available"],
            "pricing_model": tracker["openai_pricing_model"],
        },
        "total_cost": tracker["total_cost"],
    }

# --- 1. è¼”åŠ©å‡½å¼ï¼šSRT æ™‚é–“æ ¼å¼åŒ– ---
def format_timestamp(seconds):
    """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ ¼å¼ (00:00:00,000)"""
    if seconds is None:
        return "00:00:00,000"
    try:
        total_millis = int(round(float(seconds) * 1000))
    except (TypeError, ValueError):
        return "00:00:00,000"

    if total_millis < 0:
        total_millis = 0

    hours, remainder = divmod(total_millis, 3600 * 1000)
    minutes, remainder = divmod(remainder, 60 * 1000)
    secs, millis = divmod(remainder, 1000)
    return f"{hours:02}:{minutes:02}:{secs:02},{millis:03}"

# --- 2. ElevenLabs API å‘¼å« ---
def transcribe_audio(
    file_obj,
    api_key,
    language_code=None,
    model_id="scribe_v2",
    diarize=False,
    keyterms=None,
    timeout=(30, 1800),
    max_retries=2,
    retry_backoff=5
):
    """
    ä½¿ç”¨ ElevenLabs Scribe æ¨¡å‹é€²è¡Œè½‰éŒ„ã€‚
    æ”¯æ´ scribe_v1 / scribe_v2ï¼Œä»¥åŠ keyterms å’Œ diarizeã€‚
    """
    logger.info(f"Starting transcription with ElevenLabs {model_id}...")
    url = "https://api.elevenlabs.io/v1/speech-to-text"
    headers = {
        "xi-api-key": api_key
    }
    data = [
        ("model_id", model_id),
        ("tag_audio_events", "false"),
        ("timestamps_granularity", "character"),
    ]
    if language_code and language_code != "auto":
        data.append(("language_code", language_code))
    if diarize:
        data.append(("diarize", "true"))
    if keyterms:
        for term in keyterms[:100]:
            data.append(("keyterms", term[:50]))

    try:
        file_obj.seek(0)
    except Exception:
        pass

    mime_type = getattr(file_obj, "type", None) or get_mime_type(file_obj.name)
    files = {
        "file": (file_obj.name, file_obj, mime_type)
    }
    
    last_error = None
    for attempt in range(max_retries + 1):
        try:
            try:
                file_obj.seek(0)
            except Exception:
                pass

            response = requests.post(url, headers=headers, data=data, files=files, timeout=timeout)
            response.raise_for_status()

            logger.info("Transcription successful.")
            return response.json()
        except (requests.Timeout, requests.ConnectionError) as e:
            last_error = e
            if attempt < max_retries:
                sleep_seconds = retry_backoff * (2 ** attempt)
                logger.warning(f"Upload timeout/connection error. Retrying in {sleep_seconds}s (attempt {attempt + 1}/{max_retries})")
                time.sleep(sleep_seconds)
                continue
            break
        except requests.RequestException as e:
            last_error = e
            break

    resp_text = ""
    if getattr(last_error, "response", None) is not None:
        resp_text = last_error.response.text or ""
    error_msg = f"ElevenLabs API Error: {resp_text or str(last_error)}"
    logger.error(error_msg)
    raise Exception(error_msg) from last_error

# --- 3. OpenAI API å‘¼å« (æ–·å¥) ---

# å®šç¾©æ–·å¥è¼¸å‡ºçš„ JSON Schema
SEGMENTATION_SCHEMA = {
    "type": "object",
    "properties": {
        "lines": {
            "type": "array",
            "description": "æ–·å¥å¾Œçš„å­—å¹•è¡Œé™£åˆ—",
            "items": {
                "type": "string",
                "description": "å–®è¡Œå­—å¹•æ–‡å­—"
            }
        }
    },
    "required": ["lines"],
    "additionalProperties": False
}

# åˆ†æ‰¹è™•ç†è¨­å®š
SEGMENTATION_BATCH_MAX_CHARS = 600
SEGMENTATION_BATCH_DELIMITERS = set("ã€‚ï¼ï¼Ÿ!?.ï¼›;")
SEGMENTATION_BATCH_SOFT_DELIMITERS = set("ï¼Œ,ã€ \t")

# Few-shot ç¯„ä¾‹
FEW_SHOT_EXAMPLES = {
    "youtube": {
        "input": "ä»Šå¤©æˆ‘æƒ³è·Ÿå¤§å®¶èŠä¸€ä¸‹é—œæ–¼äººå·¥æ™ºæ…§çš„ç™¼å±•å…¶å¯¦æœ€è¿‘é€™å¹¾å¹´AIçš„é€²æ­¥çœŸçš„éå¸¸å¿«å¾èªéŸ³è¾¨è­˜åˆ°åœ–åƒç”Ÿæˆæ¯ä¸€å€‹é ˜åŸŸéƒ½æœ‰çªç ´æ€§çš„è®ŠåŒ–",
        "output": [
            "ä»Šå¤©æˆ‘æƒ³è·Ÿå¤§å®¶èŠä¸€ä¸‹",
            "é—œæ–¼äººå·¥æ™ºæ…§çš„ç™¼å±•",
            "å…¶å¯¦æœ€è¿‘é€™å¹¾å¹´",
            "AIçš„é€²æ­¥çœŸçš„éå¸¸å¿«",
            "å¾èªéŸ³è¾¨è­˜åˆ°åœ–åƒç”Ÿæˆ",
            "æ¯ä¸€å€‹é ˜åŸŸéƒ½æœ‰çªç ´æ€§çš„è®ŠåŒ–"
        ]
    },
    "tiktok": {
        "input": "ä»Šå¤©æˆ‘æƒ³è·Ÿå¤§å®¶èŠä¸€ä¸‹é—œæ–¼äººå·¥æ™ºæ…§çš„ç™¼å±•å…¶å¯¦æœ€è¿‘é€™å¹¾å¹´AIçš„é€²æ­¥çœŸçš„éå¸¸å¿«å¾èªéŸ³è¾¨è­˜åˆ°åœ–åƒç”Ÿæˆæ¯ä¸€å€‹é ˜åŸŸéƒ½æœ‰çªç ´æ€§çš„è®ŠåŒ–",
        "output": [
            "ä»Šå¤©æˆ‘æƒ³è·Ÿå¤§å®¶",
            "èŠä¸€ä¸‹",
            "é—œæ–¼äººå·¥æ™ºæ…§",
            "çš„ç™¼å±•",
            "å…¶å¯¦æœ€è¿‘é€™å¹¾å¹´",
            "AIçš„é€²æ­¥",
            "çœŸçš„éå¸¸å¿«",
            "å¾èªéŸ³è¾¨è­˜",
            "åˆ°åœ–åƒç”Ÿæˆ",
            "æ¯ä¸€å€‹é ˜åŸŸéƒ½æœ‰",
            "çªç ´æ€§çš„è®ŠåŒ–"
        ]
    }
}

def parse_lines_from_json(raw_text):
    """è§£æ JSON ä¸¦å–å¾— lines é™£åˆ—ï¼Œå¤±æ•—å›å‚³ None"""
    try:
        result = json.loads(raw_text)
    except (TypeError, ValueError, json.JSONDecodeError):
        return None

    if not isinstance(result, dict):
        return None

    lines = result.get("lines")
    if not isinstance(lines, list):
        return None

    return [str(line) for line in lines]

def split_text_into_batches(text, max_chars=SEGMENTATION_BATCH_MAX_CHARS):
    """å°‡é•·æ–‡æŒ‰è‡ªç„¶æ–·é»åˆ‡åˆ†ç‚ºå¤šå€‹ batchï¼Œä¿è­‰æ¯æ®µä¸è¶…é max_chars"""
    if len(text) <= max_chars:
        return [text]

    batches = []
    remaining = text

    while remaining:
        if len(remaining) <= max_chars:
            batches.append(remaining)
            break

        split_pos = -1
        for i in range(min(max_chars, len(remaining)) - 1, max_chars // 2, -1):
            if remaining[i] in SEGMENTATION_BATCH_DELIMITERS:
                split_pos = i + 1
                break

        if split_pos == -1:
            for i in range(min(max_chars, len(remaining)) - 1, max_chars // 2, -1):
                if remaining[i] in SEGMENTATION_BATCH_SOFT_DELIMITERS:
                    split_pos = i + 1
                    break

        if split_pos == -1:
            split_pos = max_chars

        batches.append(remaining[:split_pos])
        remaining = remaining[split_pos:]

    return batches

# å°é½Šæ¼”ç®—æ³•è¨­å®š
NORMALIZE_IGNORE_CHARS = set(" \t\n\rï¼Œã€‚ï¼Ÿï¼ï¼šï¼›ã€,.?!:;\"'()ï¼ˆï¼‰[]{}-â€”ï¼~ï½ã€Šã€‹")
ALIGNMENT_SEARCH_WINDOW = 50
ALIGNMENT_FALLBACK_CHAR_DURATION = 0.25
ALIGNMENT_FALLBACK_CHARS_PER_SEC = 4.0

def _content_length(text):
    """è¨ˆç®—å¿½ç•¥æ¨™é»å¾Œçš„å¯¦éš›å…§å®¹å­—æ•¸"""
    return sum(1 for c in text if c not in NORMALIZE_IGNORE_CHARS)

def _normalize_text(text):
    """æ­£è¦åŒ–æ–‡å­—ç”¨æ–¼æ¯”å°ï¼ˆå¿½ç•¥æ¨™é»ã€ç©ºç™½ï¼Œçµ±ä¸€å°å¯«ï¼‰"""
    return "".join(c for c in text if c not in NORMALIZE_IGNORE_CHARS).lower()

def validate_and_fix_lines(lines, max_chars, original_text, client, model, system_prompt, reasoning_effort=None):
    """å¾Œé©—è­‰ï¼šè¶…é•·è¡Œé€å› LLM äºŒæ¬¡æ–·å¥ï¼Œé©—è­‰å­—å…ƒå®Œæ•´æ€§ã€‚å›å‚³ (fixed_lines, extra_usage)"""
    extra_usage = {"input_tokens": 0, "output_tokens": 0}

    fixed_lines = []
    for line in lines:
        line = line.strip()
        if not line:
            continue

        if _content_length(line) <= max_chars:
            fixed_lines.append(line)
            continue

        logger.info(f"Line too long ({_content_length(line)} chars > {max_chars}), re-segmenting with LLM: {line[:30]}...")
        try:
            reseg_lines, reseg_usage = call_llm_segmentation(
                client, model, system_prompt, line, reasoning_effort
            )
            extra_usage["input_tokens"] += reseg_usage["input_tokens"]
            extra_usage["output_tokens"] += reseg_usage["output_tokens"]

            still_too_long = any(_content_length(l) > max_chars for l in reseg_lines)
            if not reseg_lines or still_too_long:
                logger.warning("Re-segmentation still produced long lines, keeping original.")
                fixed_lines.append(line)
            else:
                fixed_lines.extend(reseg_lines)
        except Exception as e:
            logger.warning(f"Re-segmentation failed, keeping original line: {e}")
            fixed_lines.append(line)

    original_norm = _normalize_text(original_text)
    result_norm = _normalize_text("".join(fixed_lines))

    if original_norm != result_norm:
        diff = len(original_norm) - len(result_norm)
        logger.warning(f"Character mismatch after segmentation: original={len(original_norm)}, result={len(result_norm)}, diff={diff}")

    return fixed_lines, extra_usage

def build_segmentation_prompt(max_chars, subtitle_style, segmentation_prompt):
    """å»ºæ§‹å« few-shot ç¯„ä¾‹çš„æ–·å¥ system prompt"""
    if subtitle_style == "tiktok":
        style_hint = f"æ¯è¡Œå­—å¹•è¦çŸ­ï¼Œé€šå¸¸ 3-8 å€‹å­—ï¼Œä¸è¶…é {max_chars} å€‹å­—ã€‚"
    else:
        style_hint = f"æ¯è¡Œå­—å¹•ä¸è¶…é {max_chars} å€‹ä¸­æ–‡å­—ã€‚"

    example = FEW_SHOT_EXAMPLES[subtitle_style]
    example_output = json.dumps({"lines": example["output"]}, ensure_ascii=False)

    system_prompt = (
        "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å­—å¹•ç·¨è¼¯å“¡ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡è¼¸å…¥çš„é€å­—ç¨¿é‡æ–°æ–·å¥ï¼Œä½¿å…¶ç¬¦åˆå­—å¹•é–±è®€ç¿’æ…£ã€‚\n"
        "è¦å‰‡ï¼š\n"
        f"1. {style_hint}\n"
        "2. é‡è¦ï¼šè«‹åš´æ ¼ä¿æŒè¼¸å…¥æ–‡æœ¬çš„åŸå§‹å­—å…ƒï¼ˆåŒ…æ‹¬ç¹ç°¡é«”ï¼‰ï¼Œä¸è¦é€²è¡Œç¹ç°¡è½‰æ›ï¼Œåƒ…é€²è¡Œæ–·å¥ã€‚çµ•å°ä¸è¦æ”¹å¯«æ–‡å­—å…§å®¹ï¼Œä¸è¦åˆªæ¸›å­—ï¼Œä¸è¦å¢åŠ å­—ï¼ˆé™¤äº†æ¨™é»ç¬¦è™Ÿï¼‰ã€‚\n"
        "3. è«‹ä¾ç…§èªæ°£å’Œèªæ„é€²è¡Œæ›è¡Œã€‚\n"
        "4. å¦‚æœé‡åˆ°èªæ°£è©ï¼ˆå¦‚ï¼šå•¦ã€å–”ã€è€¶ï¼‰ï¼Œè«‹ä¿ç•™ã€‚\n"
        "5. è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œå°‡æ¯è¡Œå­—å¹•æ”¾å…¥ lines é™£åˆ—ä¸­ã€‚\n"
        "\n"
        "--- ç¯„ä¾‹ ---\n"
        f"è¼¸å…¥ï¼š{example['input']}\n"
        f"è¼¸å‡ºï¼š{example_output}\n"
        "--- ç¯„ä¾‹çµæŸ ---"
    )

    if segmentation_prompt:
        system_prompt += f"\né¡å¤–æŒ‡ä»¤: {segmentation_prompt}"

    return system_prompt

def call_llm_segmentation(client, model, system_prompt, text, reasoning_effort=None):
    """å‘¼å« LLM é€²è¡Œå–®æ¬¡æ–·å¥ï¼Œå«ä¸‰å±¤ fallbackã€‚å›å‚³ (lines, usage)"""
    usage = {"input_tokens": 0, "output_tokens": 0}

    try:
        response_kwargs = {
            "model": model,
            "input": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ],
            "text": {
                "format": {
                    "type": "json_schema",
                    "name": "segmentation_output",
                    "strict": True,
                    "schema": SEGMENTATION_SCHEMA
                }
            },
            "temperature": 0,
        }
        if reasoning_effort and model.startswith("gpt-5"):
            response_kwargs["reasoning"] = {"effort": reasoning_effort}

        response = client.responses.create(**response_kwargs)

        if hasattr(response, 'usage') and response.usage:
            usage["input_tokens"] += getattr(response.usage, 'input_tokens', 0)
            usage["output_tokens"] += getattr(response.usage, 'output_tokens', 0)

        lines = parse_lines_from_json(response.output_text)
        if lines is None:
            raise ValueError("Responses API JSON parsing failed")

        return lines, usage

    except Exception as e:
        logger.warning(f"Responses API failed, falling back to Responses JSON-object mode: {str(e)}")

        try:
            response = client.responses.create(
                model=model,
                input=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                temperature=0,
                text={"format": {"type": "json_object"}}
            )

            if hasattr(response, 'usage') and response.usage:
                usage["input_tokens"] += getattr(response.usage, 'input_tokens', 0)
                usage["output_tokens"] += getattr(response.usage, 'output_tokens', 0)

            lines = parse_lines_from_json(response.output_text)
            if lines is None:
                raise ValueError("Responses JSON-object parsing failed")

            return lines, usage

        except Exception as e2:
            logger.warning(f"Responses JSON-object mode failed, using Chat plain text fallback: {str(e2)}")

            plain_prompt = system_prompt.replace(
                "5. è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œå°‡æ¯è¡Œå­—å¹•æ”¾å…¥ lines é™£åˆ—ä¸­ã€‚",
                "5. è¼¸å‡ºæ ¼å¼ç‚ºç´”æ–‡å­—ï¼Œè¡Œèˆ‡è¡Œä¹‹é–“ç”¨æ›è¡Œç¬¦è™Ÿåˆ†éš”ã€‚"
            )
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": plain_prompt},
                    {"role": "user", "content": text}
                ],
                temperature=0
            )

            if response.usage:
                usage["input_tokens"] += response.usage.prompt_tokens
                usage["output_tokens"] += response.usage.completion_tokens

            raw_lines = response.choices[0].message.content.strip().split("\n")
            return [l.strip() for l in raw_lines if l.strip()], usage

def segment_text_with_llm(full_text, api_key, model, max_chars, segmentation_prompt,
                          reasoning_effort="none", subtitle_style="youtube"):
    """è«‹ LLM å°‡é•·æ–‡å­—åˆ‡åˆ†ç‚ºå­—å¹•è¡Œï¼Œå«åˆ†æ‰¹è™•ç†èˆ‡å¾Œé©—è­‰ã€‚

    Returns:
        tuple: (segmented_text, usage_dict) - æ–·å¥æ–‡å­—å’Œä½¿ç”¨é‡è³‡è¨Š
    """
    logger.info(f"Starting LLM segmentation. Model: {model}, Max chars: {max_chars}, Style: {subtitle_style}")
    logger.info(f"Reasoning effort: {reasoning_effort}")

    total_usage = {"input_tokens": 0, "output_tokens": 0}

    if not full_text or not full_text.strip():
        return "", total_usage

    client = openai.OpenAI(api_key=api_key)
    system_prompt = build_segmentation_prompt(max_chars, subtitle_style, segmentation_prompt)

    batches = split_text_into_batches(full_text.strip())
    logger.info(f"Split text into {len(batches)} batch(es) for segmentation.")

    all_lines = []
    for i, batch in enumerate(batches):
        logger.info(f"Processing batch {i + 1}/{len(batches)} ({len(batch)} chars)...")

        lines, usage = call_llm_segmentation(
            client, model, system_prompt, batch, reasoning_effort
        )

        total_usage["input_tokens"] += usage["input_tokens"]
        total_usage["output_tokens"] += usage["output_tokens"]
        all_lines.extend(lines)

    all_lines, fix_usage = validate_and_fix_lines(
        all_lines, max_chars, full_text, client, model, system_prompt, reasoning_effort
    )
    total_usage["input_tokens"] += fix_usage["input_tokens"]
    total_usage["output_tokens"] += fix_usage["output_tokens"]

    track_openai_cost(model, total_usage["input_tokens"], total_usage["output_tokens"])
    logger.info(f"Segmentation complete: {len(all_lines)} lines, tokens: in={total_usage['input_tokens']}, out={total_usage['output_tokens']}")

    return "\n".join(all_lines), total_usage

# --- 4. æ ¸å¿ƒé‚è¼¯ï¼šAlignment (å°é½Š) ---
def align_transcript(raw_api_data, llm_segmented_text):
    """
    å°‡ LLM åˆ†å¥½è¡Œçš„æ–‡å­— (ç„¡æ™‚é–“) èˆ‡ ElevenLabs çš„ Character (æœ‰æ™‚é–“) é€²è¡Œå°é½Šã€‚
    ä½¿ç”¨å¼·å¥çš„æ­£è¦åŒ–éŒ¨é»åŒ¹é… (Robust Normalized Anchor Matching)ã€‚
    """
    logger.info("Starting alignment process (Robust Logic)...")

    # Debug: è¨˜éŒ„ API å›å‚³çš„çµæ§‹
    logger.info(f"API response keys: {list(raw_api_data.keys())}")

    # --- 1. æå–åŸå§‹å­—å…ƒè³‡è¨Š (Raw Characters Extraction) ---
    raw_chars = []
    
    # æ–¹æ³• 1: å˜—è©¦å¾ words -> characters çµæ§‹å–å¾—
    words_data = raw_api_data.get('words', [])
    
    for word in words_data:
        if not isinstance(word, dict):
            continue

        # æª¢æŸ¥æ˜¯å¦æœ‰ characters é™£åˆ—
        if 'characters' in word and word['characters'] is not None:
            for char_obj in word['characters']:
                if (
                    isinstance(char_obj, dict)
                    and 'text' in char_obj
                    and 'start' in char_obj
                    and 'end' in char_obj
                ):
                    raw_chars.append(char_obj)
        # å‚™ç”¨ï¼šå¦‚æœæ²’æœ‰ charactersï¼Œä½† word æœ¬èº«æœ‰æ™‚é–“è³‡è¨Š
        elif 'text' in word and 'start' in word and 'end' in word:
            word_text = word.get('text', '')
            try:
                word_start = float(word.get('start', 0) or 0)
                word_end = float(word.get('end', 0) or 0)
            except (TypeError, ValueError):
                continue
                
            if word_end < word_start:
                word_start, word_end = word_end, word_start

            if word_text and len(word_text) > 0:
                duration_per_char = (word_end - word_start) / len(word_text) if len(word_text) > 0 else 0
                for idx, char in enumerate(word_text):
                    raw_chars.append({
                        'text': char,
                        'start': word_start + idx * duration_per_char,
                        'end': word_start + (idx + 1) * duration_per_char if idx < len(word_text) - 1 else word_end
                    })

    logger.info(f"Total raw characters extracted: {len(raw_chars)}")

    # å¦‚æœç„¡æ³•å–å¾—å­—å…ƒç´šæ™‚é–“æˆ³ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼šæ ¹æ“šéŸ³è¨Šé•·åº¦ä¼°ç®—
    if len(raw_chars) == 0:
        logger.warning("No character-level timestamps available. Using fallback time estimation.")
        lines = llm_segmented_text.split('\n')
        srt_output = []
        
        audio_duration = raw_api_data.get('audio_duration', None)
        if audio_duration is None:
            total_text_len = sum(len(line.strip()) for line in lines if line.strip())
            audio_duration = total_text_len / ALIGNMENT_FALLBACK_CHARS_PER_SEC if total_text_len > 0 else 60.0
        
        total_chars_count = sum(len(line.strip()) for line in lines if line.strip())
        char_duration = audio_duration / total_chars_count if total_chars_count > 0 else ALIGNMENT_FALLBACK_CHAR_DURATION

        current_time = 0.0
        line_index = 0
        for line in lines:
            clean_line = line.strip()
            if not clean_line:
                continue

            line_index += 1
            line_duration = len(clean_line) * char_duration
            srt_output.append({
                "index": line_index,
                "start": format_timestamp(current_time),
                "end": format_timestamp(current_time + line_duration),
                "text": clean_line
            })
            current_time += line_duration
            
        return srt_output, total_chars_count, total_chars_count

    # --- 2. é è™•ç†ï¼šæ­£è¦åŒ–åŸå§‹åºåˆ— ---
    searchable_raw = []
    
    for rc in raw_chars:
        c = rc['text']
        if c not in NORMALIZE_IGNORE_CHARS:
            searchable_raw.append({
                'char': c.lower(),
                'start': rc['start'],
                'end': rc['end']
            })
    
    # --- 3. å°é½Šé‚è¼¯ (Robust Anchor Matching) ---
    lines = llm_segmented_text.split('\n')
    srt_output = []
    
    curr_search_idx = 0
    total_raw_len = len(searchable_raw)
    matched_count = 0
    total_llm_chars = 0
    last_valid_end = 0.0

    for line in lines:
        clean_line = line.strip()
        if not clean_line:
            continue
            
        line_chars = [c.lower() for c in clean_line if c not in NORMALIZE_IGNORE_CHARS]
        total_llm_chars += len(line_chars)
        
        if not line_chars:
            continue

        line_start_time = None
        line_end_time = None
        
        temp_idx = curr_search_idx
        line_matches = 0
        
        first_match_start = None
        last_match_end = None

        for lc in line_chars:
            # è¦–çª—æœå°‹ (Window Search)
            search_window = ALIGNMENT_SEARCH_WINDOW
            found_at = -1
            
            for offset in range(search_window):
                if temp_idx + offset >= total_raw_len:
                    break
                
                if searchable_raw[temp_idx + offset]['char'] == lc:
                    found_at = temp_idx + offset
                    break
            
            if found_at != -1:
                if first_match_start is None:
                    first_match_start = searchable_raw[found_at]['start']
                last_match_end = searchable_raw[found_at]['end']
                
                temp_idx = found_at + 1
                line_matches += 1
                matched_count += 1
        
        if line_matches > 0:
            line_start_time = first_match_start
            line_end_time = last_match_end
            
            # æ›´æ–°å…¨åŸŸæœå°‹æŒ‡æ¨™
            curr_search_idx = temp_idx
            last_valid_end = line_end_time
        else:
            # è©²è¡Œå®Œå…¨æœªåŒ¹é… (Fallback)
            est_duration = len(line_chars) * ALIGNMENT_FALLBACK_CHAR_DURATION
            line_start_time = last_valid_end
            line_end_time = last_valid_end + est_duration
            last_valid_end = line_end_time

        srt_output.append({
            "index": len(srt_output) + 1,
            "start": format_timestamp(line_start_time),
            "end": format_timestamp(line_end_time),
            "text": clean_line
        })

    logger.info(f"Alignment completed. Matched {matched_count}/{total_llm_chars} characters.")
    return srt_output, matched_count, total_llm_chars

def parse_keyword_rules(raw_text):
    """è§£æ keyword ä¿®æ­£èˆ‡ä¿ç•™è©"""
    replacements = []
    keep_terms = []

    if not raw_text:
        return replacements, keep_terms

    for line in raw_text.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue

        separator = None
        for sep in ("=>", "->", "="):
            if sep in line:
                separator = sep
                break

        if separator:
            src, dst = line.split(separator, 1)
            src = src.strip()
            dst = dst.strip()
            if src and dst:
                replacements.append((src, dst))
        else:
            keep_terms.append(line)

    return replacements, keep_terms

def apply_replacements_to_text(text, replacements):
    if not replacements:
        return text

    updated = text
    for src, dst in replacements:
        if src:
            updated = updated.replace(src, dst)
    return updated

def apply_replacements_to_lines(lines, replacements):
    if not replacements:
        return lines
    return [apply_replacements_to_text(line, replacements) for line in lines]

def set_srt_texts(srt_data, lines):
    if len(srt_data) != len(lines):
        logger.warning(f"SRT line count mismatch: srt={len(srt_data)}, lines={len(lines)}. Updating available lines only.")
        st.warning(f"âš ï¸ æ ¡æ­£/ç¿»è­¯å¾Œè¡Œæ•¸ä¸ä¸€è‡´ (åŸ {len(srt_data)} è¡Œ â†’ {len(lines)} è¡Œ)ï¼Œéƒ¨åˆ†å­—å¹•å¯èƒ½æœªæ›´æ–°ã€‚")

    for item, line in zip(srt_data, lines):
        item["text"] = line
    return srt_data

def split_lines_into_batches(lines, max_chars=3000, max_lines=40):
    batches = []
    current = []
    current_len = 0

    for line in lines:
        line_len = len(line)
        if current and (len(current) >= max_lines or current_len + line_len > max_chars):
            batches.append(current)
            current = [line]
            current_len = line_len
        else:
            current.append(line)
            current_len += line_len

    if current:
        batches.append(current)
    return batches

def parse_json_data_list(raw_text):
    try:
        result = json.loads(raw_text)
    except (TypeError, ValueError, json.JSONDecodeError):
        return None

    if not isinstance(result, dict):
        return None

    data = result.get("data")
    if not isinstance(data, list):
        return None

    return [str(item) for item in data]

def build_glossary_instruction(replacements, keep_terms):
    lines = []
    if replacements:
        lines.append("è«‹ä½¿ç”¨ä»¥ä¸‹å°æ‡‰è©å½™ï¼š")
        for src, dst in replacements:
            lines.append(f"{src} -> {dst}")
    if keep_terms:
        lines.append("è«‹åŸæ¨£ä¿ç•™ä»¥ä¸‹è©å½™ï¼š")
        lines.extend(keep_terms)
    return "\n".join(lines)

def llm_transform_lines(lines, api_key, model, system_prompt, temperature=0.2):
    if not lines:
        return lines, {"input_tokens": 0, "output_tokens": 0}

    client = openai.OpenAI(api_key=api_key)
    batches = split_lines_into_batches(lines)
    output_lines = []
    usage = {"input_tokens": 0, "output_tokens": 0}

    for batch in batches:
        payload = json.dumps({"data": batch}, ensure_ascii=False)
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": payload}
                ],
                temperature=temperature,
                response_format={"type": "json_object"}
            )

            if response.usage:
                usage["input_tokens"] += response.usage.prompt_tokens
                usage["output_tokens"] += response.usage.completion_tokens

            parsed = parse_json_data_list(response.choices[0].message.content)
            if not parsed or len(parsed) != len(batch):
                logger.warning("LLM output length mismatch; using original batch.")
                output_lines.extend(batch)
                continue

            output_lines.extend(parsed)

        except Exception as e:
            logger.error(f"LLM batch failed: {str(e)}")
            output_lines.extend(batch)

    if usage["input_tokens"] or usage["output_tokens"]:
        track_openai_cost(model, usage["input_tokens"], usage["output_tokens"])

    return output_lines, usage

def correct_lines_with_llm(lines, api_key, model, output_style, subtitle_style, replacements, keep_terms):
    style_hint = "å¿ å¯¦å‘ˆç¾ï¼šåªä¿®æ­£æ˜é¡¯éŒ¯å­—ï¼Œä¸æ”¹å¯«èªå¥" if output_style == "faithful" else "é€šé †è‡ªç„¶ï¼šå¯å°å¹…èª¿æ•´èªåºä½†ä¸å¢åˆªè³‡è¨Š"
    length_hint = "ä¿æŒçŸ­å¥ã€ç¯€å¥å¿«" if subtitle_style == "tiktok" else "ä¿æŒå­—å¹•é–±è®€èˆ’é©"
    glossary = build_glossary_instruction(replacements, keep_terms)

    system_prompt = (
        "ä½ æ˜¯å­—å¹•æ ¡å°å“¡ï¼Œè«‹ä¿®æ­£è½‰éŒ„éŒ¯å­—èˆ‡è½éŒ¯è©ã€‚\n"
        f"è¼¸å‡ºé¢¨æ ¼ï¼š{style_hint}\n"
        f"å­—å¹•é¢¨æ ¼ï¼š{length_hint}\n"
        "è¦å‰‡ï¼š\n"
        "1. æ¯è¡Œç¨ç«‹è™•ç†ï¼Œä¸åˆä½µã€ä¸æ‹†è¡Œã€‚\n"
        "2. ä¿ç•™åŸæ„ï¼Œä¸å¢åŠ æˆ–åˆªæ¸›è³‡è¨Šã€‚\n"
        "3. åƒ…è¼¸å‡º JSON æ ¼å¼ï¼š{\"data\": [...]}ï¼Œé•·åº¦éœ€èˆ‡è¼¸å…¥ä¸€è‡´ã€‚\n"
    )
    if glossary:
        system_prompt += f"\n{glossary}"

    return llm_transform_lines(lines, api_key, model, system_prompt, temperature=0.1)

def translate_lines_with_llm(lines, api_key, model, target_language, output_style, subtitle_style, replacements, keep_terms):
    style_hint = "å¿ å¯¦å‘ˆç¾ï¼šç›¡é‡ç›´è­¯ï¼Œä¿ç•™èªæ°£" if output_style == "faithful" else "é€šé †è‡ªç„¶ï¼šå¯é©åº¦æ½¤é£¾ä½†ä¸æ”¹è®Šæ„æ€"
    length_hint = "ä¿æŒçŸ­å¥ã€ç¯€å¥å¿«" if subtitle_style == "tiktok" else "ä¿æŒå­—å¹•é–±è®€èˆ’é©"
    glossary = build_glossary_instruction(replacements, keep_terms)

    system_prompt = (
        f"ä½ æ˜¯å­—å¹•ç¿»è­¯å“¡ï¼Œè«‹å°‡æ¯è¡Œå­—å¹•ç¿»è­¯æˆ{target_language}ã€‚\n"
        f"è¼¸å‡ºé¢¨æ ¼ï¼š{style_hint}\n"
        f"å­—å¹•é¢¨æ ¼ï¼š{length_hint}\n"
        "è¦å‰‡ï¼š\n"
        "1. æ¯è¡Œç¨ç«‹ç¿»è­¯ï¼Œä¸åˆä½µã€ä¸æ‹†è¡Œã€‚\n"
        "2. ä¿ç•™åŸæ„ï¼Œä¸å¢åŠ æˆ–åˆªæ¸›è³‡è¨Šã€‚\n"
        "3. åƒ…è¼¸å‡º JSON æ ¼å¼ï¼š{\"data\": [...]}ï¼Œé•·åº¦éœ€èˆ‡è¼¸å…¥ä¸€è‡´ã€‚\n"
    )
    if glossary:
        system_prompt += f"\n{glossary}"

    return llm_transform_lines(lines, api_key, model, system_prompt, temperature=0.2)

def convert_lines_to_traditional(lines):
    """ä½¿ç”¨ OpenCC å°‡å­—å¹•è½‰æ›ç‚ºå°ç£ç¹é«”ä¸­æ–‡"""
    try:
        converter = opencc.OpenCC('s2twp')
        return [converter.convert(line) for line in lines]
    except Exception as e:
        logger.error(f"OpenCC conversion error: {e}")
        return lines

def clean_subtitle_text(text):
    """æ¸…ç†å­—å¹•æ–‡å­—ï¼Œç§»é™¤ä¸éœ€è¦çš„æ¨™é»ç¬¦è™Ÿ"""
    if not text:
        return text

    # ç§»é™¤é–‹é ­çš„ - æˆ– â€”
    text = text.lstrip('-â€”ï¼')

    # ç§»é™¤çµå°¾çš„æ¨™é»ç¬¦è™Ÿï¼ˆé€—è™Ÿã€å¥è™Ÿã€é “è™Ÿã€ç ´æŠ˜è™Ÿç­‰ï¼Œä¿ç•™å•è™Ÿå’Œæ„Ÿæ­è™Ÿï¼‰
    trailing_punctuation = 'ï¼Œã€‚ã€,.-â€”ï¼;ï¼›:ï¼š'
    text = text.rstrip(trailing_punctuation)

    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
    text = text.strip()

    return text

def generate_srt_string(srt_data, clean_text=True):
    parts = []
    for item in srt_data:
        cleaned_text = clean_subtitle_text(item['text']) if clean_text else item['text']
        parts.append(f"{item['index']}\n{item['start']} --> {item['end']}\n{cleaned_text}\n")
    return "\n".join(parts) + "\n" if parts else ""

# --- 5. Streamlit UI ---
st.set_page_config(page_title="AI å­—å¹•ç”Ÿæˆå™¨ (ElevenLabs + OpenAI)", layout="wide")

st.title("ğŸ¬ AI å­—å¹•ç”Ÿæˆå™¨ (Word-Level Timestamp)")
st.markdown("çµåˆ **ElevenLabs Scribe** çš„ç²¾æº–æ™‚é–“æˆ³è¨˜èˆ‡ **OpenAI** çš„èªæ„æ–·å¥èƒ½åŠ›ã€‚")

# åˆå§‹åŒ–è²»ç”¨è¿½è¹¤å™¨
init_cost_tracker()

# Sidebar: è¨­å®š
with st.sidebar:
    st.header("ğŸ”‘ API Keys")
    el_key = st.text_input("ElevenLabs API Key", type="password")
    oa_key = st.text_input("OpenAI API Key", type="password")

    # API é©—è­‰æŒ‰éˆ•
    if el_key or oa_key:
        if st.button("ğŸ” é©—è­‰ API Keys"):
            with st.spinner("é©—è­‰ä¸­..."):
                # é©—è­‰ ElevenLabs
                if el_key:
                    el_result = validate_elevenlabs_key(el_key)
                    if el_result["valid"]:
                        st.success(f"âœ… ElevenLabs: {el_result['tier']}")
                        st.caption(f"å‰©é¤˜é¡åº¦: {el_result['remaining_characters']:,} å­—å…ƒ")
                    else:
                        st.error(f"âŒ ElevenLabs: {el_result['error']}")

                # é©—è­‰ OpenAI
                if oa_key:
                    oa_result = validate_openai_key(oa_key)
                    if oa_result["valid"]:
                        st.success("âœ… OpenAI: API Key æœ‰æ•ˆ")
                    else:
                        st.error(f"âŒ OpenAI: {oa_result['error']}")

    st.header("âš™ï¸ åƒæ•¸è¨­å®š")

    # å­—å¹•é¢¨æ ¼é¸æ“‡
    style_options = {
        "YouTube (å®Œæ•´èªå¥)": "youtube",
        "TikTok (çŸ­å¥å¿«ç¯€å¥)": "tiktok",
    }
    selected_style = st.selectbox("å­—å¹•é¢¨æ ¼", list(style_options.keys()), index=0)
    subtitle_style = style_options[selected_style]

    # æ ¹æ“šé¢¨æ ¼èª¿æ•´é è¨­å­—æ•¸
    default_chars = 8 if subtitle_style == "tiktok" else 16
    max_range = 15 if subtitle_style == "tiktok" else 30
    min_range = 3 if subtitle_style == "tiktok" else 10

    max_chars = st.slider("æ¯è¡Œæœ€å¤§å­—æ•¸", min_range, max_range, default_chars)

    # æ¨¡å‹é¸æ“‡ï¼ˆé è¨­ GPT-4.1ï¼‰
    model_options = {
        "GPT-4.1 (é è¨­)": "gpt-4.1",
        "GPT-4.1 mini": "gpt-4.1-mini",
        "GPT-5.2": "gpt-5.2",
        "GPT-5-mini": "gpt-5-mini",
    }
    selected_model = st.selectbox("OpenAI Model", list(model_options.keys()), index=0)
    model_choice = model_options[selected_model]

    # èªè¨€é¸æ“‡
    language_options = {
        "è‡ªå‹•åµæ¸¬": "auto",
        "ä¸­æ–‡": "zho",
        "è‹±æ–‡": "eng",
        "æ—¥æ–‡": "jpn",
        "éŸ“æ–‡": "kor",
        "ç²µèª": "yue",
    }
    selected_lang = st.selectbox("éŸ³è¨Šèªè¨€", list(language_options.keys()))
    language_code = language_options[selected_lang]

    # ElevenLabs æ¨¡å‹é¸æ“‡
    scribe_options = {
        "Scribe v2 (æ¨è–¦)": "scribe_v2",
        "Scribe v1": "scribe_v1",
    }
    selected_scribe = st.selectbox("è½‰éŒ„æ¨¡å‹", list(scribe_options.keys()), index=0)
    scribe_model = scribe_options[selected_scribe]

    enable_diarize = st.checkbox("èªªè©±è€…è¾¨è­˜ (Diarize)", value=False, help="æ¨™è¨»éŸ³è¨Šä¸­ä¸åŒèªªè©±è€…ï¼Œé©åˆå¤šäººå°è©±å ´æ™¯ã€‚")

    reasoning_effort = None
    with st.expander("é€²éšè¨­å®š"):
        custom_prompt = st.text_area("çµ¦æ–·å¥ LLM çš„é¡å¤–æŒ‡ä»¤", value="ä¿ç•™èªæ°£è©ã€‚")
        clean_punctuation = st.checkbox("æ¸…ç†å­—å¹•æ¨™é»", value=True, help="ç§»é™¤è¡Œé¦–ç ´æŠ˜è™Ÿèˆ‡è¡Œå°¾é€—è™Ÿ/å¥è™Ÿç­‰ã€‚")
        show_debug = st.checkbox("é¡¯ç¤ºèª¿è©¦è³‡è¨Š", value=False, help="é¡¯ç¤º ElevenLabs API åŸå§‹å›æ‡‰ã€‚")

        st.markdown("---")
        st.markdown("**æ–‡å­—æ ¡æ­£ / ç¿»è­¯**")

        enable_correction = st.checkbox("å­—å¹•æ ¡æ­£ï¼ˆä¿®æ­£éŒ¯å­—/é †å¥ï¼‰", value=False)

        output_style_options = {
            "å¿ å¯¦å‘ˆç¾": "faithful",
            "é€šé †è‡ªç„¶": "fluent",
        }
        selected_output_style = st.selectbox("è¼¸å‡ºé¢¨æ ¼", list(output_style_options.keys()), index=0)
        output_style = output_style_options[selected_output_style]

        translation_options = {
            "ä¸ç¿»è­¯ï¼ˆåŸæ–‡ï¼‰": "source",
            "ç¹é«”ä¸­æ–‡": "ç¹é«”ä¸­æ–‡",
            "è‹±æ–‡": "è‹±æ–‡",
            "æ—¥æ–‡": "æ—¥æ–‡",
            "é¦¬ä¾†æ–‡": "é¦¬ä¾†æ–‡",
        }
        selected_translation = st.selectbox("ç¿»è­¯èªè¨€", list(translation_options.keys()), index=0)
        target_language = translation_options[selected_translation]

        use_opencc = False
        if target_language == "ç¹é«”ä¸­æ–‡":
            use_opencc = st.checkbox("åƒ…åšç¹ç°¡è½‰æ› (OpenCC)", value=True, help="åƒ…é©ç”¨ä¸­æ–‡ä¾†æºï¼Œéç¿»è­¯ã€‚")

        keyword_rules = st.text_area(
            "Keyword ä¿®æ­£ / è©å½™è¡¨",
            value="",
            help="ä¸€è¡Œä¸€çµ„ï¼šåŸè©=ä¿®æ­£è© æˆ– åŸè©=>ç›®æ¨™è©ï¼›æ²’æœ‰ç­‰è™Ÿè¦–ç‚ºå¼·åˆ¶ä¿ç•™è©ã€‚",
            placeholder="ç‹å°æ˜=å°æ˜\nOpenAI=>OpenAI\nHokage"
        )

        st.markdown("---")
        st.markdown("**ä¸Šå‚³èˆ‡é‡è©¦**")
        connect_timeout = st.number_input("ElevenLabs é€£ç·šé€¾æ™‚ (ç§’)", min_value=5, max_value=300, value=30, step=5)
        read_timeout = st.number_input("ElevenLabs å›æ‡‰é€¾æ™‚ (ç§’)", min_value=300, max_value=7200, value=1800, step=60)
        retry_count = st.number_input("ä¸Šå‚³é‡è©¦æ¬¡æ•¸", min_value=0, max_value=5, value=2, step=1)
        retry_backoff = st.number_input("é‡è©¦ç­‰å¾…ç§’æ•¸", min_value=0, max_value=60, value=5, step=1)

        st.markdown("---")
        st.markdown("**GPT-5 å°ˆå±¬è¨­å®š**")

        if model_choice.startswith("gpt-5"):
            # æ¨ç†å¼·åº¦ (Reasoning Effort) - æ ¹æ“šæ¨¡å‹å‹•æ…‹èª¿æ•´é¸é …
            # - gpt-5.2: none, low, medium, high, xhigh
            # - gpt-5-mini: minimal, low, medium, high
            if model_choice == "gpt-5.2":
                reasoning_options = {
                    "none (æœ€å¿«)": "none",
                    "low": "low",
                    "medium": "medium",
                    "high": "high",
                    "xhigh (æœ€æ·±åº¦)": "xhigh",
                }
                help_text = "æ§åˆ¶æ¨¡å‹æ¨ç†æ·±åº¦ã€‚none æœ€å¿«ï¼Œxhigh æœ€æ·±åº¦æ€è€ƒã€‚"
            else:  # gpt-5-mini
                reasoning_options = {
                    "minimal (æœ€å¿«)": "minimal",
                    "low": "low",
                    "medium": "medium",
                    "high": "high",
                }
                help_text = "æ§åˆ¶æ¨¡å‹æ¨ç†æ·±åº¦ã€‚minimal æœ€å¿«ï¼Œhigh æœ€æ·±åº¦æ€è€ƒã€‚"

            selected_reasoning = st.selectbox(
                "æ¨ç†å¼·åº¦ (Reasoning Effort)",
                list(reasoning_options.keys()),
                index=0,
                help=help_text
            )
            reasoning_effort = reasoning_options[selected_reasoning]
        else:
            st.caption("ç›®å‰æ¨¡å‹ä¸æ”¯æ´ reasoning è¨­å®šã€‚")

# Main Area
uploaded_file = st.file_uploader("ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ (mp3, wav, m4a, flac, ogg)", type=SUPPORTED_AUDIO_TYPES)

# åˆå§‹åŒ– session state
if 'result' not in st.session_state:
    st.session_state.result = None
if 'cached_transcript' not in st.session_state:
    st.session_state.cached_transcript = None
if 'cached_file_key' not in st.session_state:
    st.session_state.cached_file_key = None

def _file_cache_key(f):
    """ç”¢ç”Ÿæª”æ¡ˆå¿«å– keyï¼ˆåç¨±+å¤§å°+å…§å®¹é›œæ¹Šï¼‰ä»¥é¿å…èª¤å‘½ä¸­"""
    try:
        digest = hashlib.sha256(f.getbuffer()).hexdigest()[:16]
    except Exception:
        digest = "nohash"
    return f"{f.name}_{f.size}_{digest}"

def _run_pipeline(raw_transcript, uploaded_file, skip_transcribe=False):
    """åŸ·è¡Œ Step 2-4 çš„å…±ç”¨æµç¨‹ï¼Œå›å‚³æ˜¯å¦æˆåŠŸ"""
    setup_logger()
    log_stream = get_log_stream()
    log_stream.truncate(0)
    log_stream.seek(0)

    st.session_state.result = None
    reset_cost_tracker()

    status = st.status("æ­£åœ¨è™•ç†ä¸­...", expanded=True)
    error_occurred = False

    try:
        full_text = raw_transcript.get('text', '')

        if not skip_transcribe:
            audio_duration = raw_transcript.get('audio_duration', 0)
            if audio_duration:
                el_cost = track_elevenlabs_cost(audio_duration)
                logger.info(f"ElevenLabs cost: ${el_cost:.4f} for {audio_duration:.2f}s audio")

        if not full_text or not full_text.strip():
            status.update(label="âŒ è½‰éŒ„çµæœç‚ºç©º", state="error")
            st.error("âŒ è½‰éŒ„çµæœç‚ºç©ºç™½ï¼Œå¯èƒ½æ˜¯éœéŸ³æª”æ¡ˆæˆ–ä¸æ”¯æ´çš„æ ¼å¼ã€‚è«‹ç¢ºèªéŸ³è¨Šå…§å®¹å¾Œé‡è©¦ã€‚")
            return

        status.write(f"ğŸ“ è½‰éŒ„æ–‡å­—ï¼šå…± {len(full_text)} å€‹å­—ã€‚")

        # Step 2: LLM Segmentation
        status.write(f"ğŸ§  æ­£åœ¨å‘¼å« {model_choice} é€²è¡Œèªæ„æ–·å¥ ({subtitle_style} é¢¨æ ¼)...")
        segmented_text, seg_usage = segment_text_with_llm(
            full_text, oa_key, model_choice, max_chars, custom_prompt,
            reasoning_effort=reasoning_effort,
            subtitle_style=subtitle_style
        )
        status.write("âœ… æ–·å¥å®Œæˆï¼")

        # Step 3: Alignment
        status.write("ğŸ”— æ­£åœ¨é€²è¡Œæ™‚é–“è»¸å°é½Š (Word/Char Level Alignment)...")
        srt_data, matched_cnt, total_cnt = align_transcript(raw_transcript, segmented_text)

        match_rate = (matched_cnt / total_cnt * 100) if total_cnt > 0 else 0
        status.write(f"ğŸ“Š å°é½ŠåŒ¹é…ç‡: {match_rate:.2f}% ({matched_cnt}/{total_cnt})")
        logger.info(f"Match rate: {match_rate:.2f}%")

        low_match_rate = match_rate < 80

        # Step 4: Keyword ä¿®æ­£ / æ ¡æ­£ / ç¿»è­¯
        srt_lines = [item["text"] for item in srt_data]
        replacements, keep_terms = parse_keyword_rules(keyword_rules)

        if replacements:
            srt_lines = apply_replacements_to_lines(srt_lines, replacements)

        if enable_correction and srt_lines:
            status.write("ğŸ§¹ æ­£åœ¨æ ¡æ­£å­—å¹•æ–‡å­—...")
            srt_lines, _ = correct_lines_with_llm(
                srt_lines, oa_key, model_choice, output_style,
                subtitle_style, replacements, keep_terms
            )
            status.write("âœ… å­—å¹•æ ¡æ­£å®Œæˆï¼")

        if target_language != "source" and srt_lines:
            if target_language == "ç¹é«”ä¸­æ–‡" and use_opencc:
                status.write("ğŸ‡¨ğŸ‡³->ğŸ‡¹ğŸ‡¼ æ­£åœ¨è½‰æ›ç‚ºç¹é«”ä¸­æ–‡ (OpenCC)...")
                srt_lines = convert_lines_to_traditional(srt_lines)
                status.write("âœ… ç¹é«”è½‰æ›å®Œæˆï¼")
            else:
                status.write(f"ğŸŒ æ­£åœ¨ç¿»è­¯å­—å¹•ç‚º {target_language}...")
                srt_lines, _ = translate_lines_with_llm(
                    srt_lines, oa_key, model_choice, target_language,
                    output_style, subtitle_style, replacements, keep_terms
                )
                status.write("âœ… ç¿»è­¯å®Œæˆï¼")

        srt_data = set_srt_texts(srt_data, srt_lines)
        srt_string = generate_srt_string(srt_data, clean_text=clean_punctuation)
        cost_summary = get_cost_summary()

        status.update(label="ğŸ‰ ä»»å‹™å®Œæˆï¼", state="complete", expanded=False)

        st.session_state.result = {
            'full_text': full_text,
            'segmented_text': segmented_text,
            'srt_string': srt_string,
            'srt_data': srt_data,
            'low_match_rate': low_match_rate,
            'filename': uploaded_file.name,
            'raw_api_response': raw_transcript,
            'cost_summary': cost_summary,
        }

    except Exception as e:
        error_occurred = True
        status.update(label="âŒ ç™¼ç”ŸéŒ¯èª¤", state="error")
        st.error(f"Error Log: {str(e)}")
        logger.error(f"Critical error: {str(e)}")

    with st.expander("ğŸ“ åŸ·è¡Œæ—¥èªŒ (Logs)", expanded=error_occurred):
        st.code(get_log_stream().getvalue())

if uploaded_file and el_key and oa_key:
    file_size_mb = uploaded_file.size / (1024 * 1024)
    file_ext = uploaded_file.name.rsplit('.', 1)[-1].upper() if '.' in uploaded_file.name else '?'

    # æª”æ¡ˆè³‡è¨Šé¡¯ç¤º
    st.caption(f"ğŸ“ **{uploaded_file.name}** â€” {file_size_mb:.1f} MB Â· {file_ext}")

    if file_size_mb > UPLOAD_MAX_SIZE_MB:
        st.error(f"âŒ æª”æ¡ˆéå¤§ ({file_size_mb:.1f} MB)ï¼Œä¸Šé™ç‚º {UPLOAD_MAX_SIZE_MB} MBã€‚")
    else:
        # æª¢æŸ¥æ˜¯å¦æœ‰åŒä¸€æª”æ¡ˆçš„å¿«å–è½‰éŒ„
        current_key = _file_cache_key(uploaded_file)
        has_cache = (
            st.session_state.cached_transcript is not None
            and st.session_state.cached_file_key == current_key
        )

        if has_cache:
            col_btn1, col_btn2 = st.columns(2)
            with col_btn1:
                btn_full = st.button("ğŸ”„ é‡æ–°è½‰éŒ„ + ç”Ÿæˆå­—å¹•", use_container_width=True)
            with col_btn2:
                btn_reseg = st.button("âœ‚ï¸ é‡æ–°æ–·å¥ï¼ˆä½¿ç”¨å¿«å–è½‰éŒ„ï¼‰", use_container_width=True,
                                      help="è·³é ElevenLabs è½‰éŒ„ï¼Œç›´æ¥ç”¨ä¸Šæ¬¡çš„è½‰éŒ„çµæœé‡æ–°æ–·å¥ã€‚çœæ™‚çœéŒ¢ã€‚")
        else:
            btn_full = st.button("é–‹å§‹ç”Ÿæˆå­—å¹•", use_container_width=True)
            btn_reseg = False

        if btn_full:
            uploaded_file.seek(0)
            _, api_keep_terms = parse_keyword_rules(keyword_rules)
            est_minutes = max(1, file_size_mb * 0.5)
            scribe_label = scribe_model.replace("_", " ").title()
            with st.spinner(f"ğŸ§ æ­£åœ¨ä¸Šå‚³è‡³ ElevenLabs é€²è¡Œè½‰éŒ„ ({scribe_label})... é ä¼°éœ€è¦ {est_minutes:.0f}-{est_minutes * 2:.0f} åˆ†é˜"):
                raw_transcript = transcribe_audio(
                    uploaded_file, el_key, language_code,
                    model_id=scribe_model,
                    diarize=enable_diarize,
                    keyterms=api_keep_terms if api_keep_terms else None,
                    timeout=(connect_timeout, read_timeout),
                    max_retries=int(retry_count),
                    retry_backoff=int(retry_backoff)
                )
            st.session_state.cached_transcript = raw_transcript
            st.session_state.cached_file_key = current_key
            _run_pipeline(raw_transcript, uploaded_file, skip_transcribe=False)

        elif btn_reseg:
            _run_pipeline(st.session_state.cached_transcript, uploaded_file, skip_transcribe=True)

    # é¡¯ç¤ºçµæœ
    if st.session_state.result:
        result = st.session_state.result

        st.markdown("---")

        # è²»ç”¨æ‘˜è¦
        if 'cost_summary' in result:
            cost = result['cost_summary']
            openai_line = (
                f"- OpenAI: {cost['openai']['input_tokens']:,} input + {cost['openai']['output_tokens']:,} output tokens = ${cost['openai']['cost']:.4f}"
                if cost['openai']['pricing_available']
                else f"- OpenAI: {cost['openai']['input_tokens']:,} input + {cost['openai']['output_tokens']:,} output tokens = N/Aï¼ˆæœªè¨­å®š {cost['openai']['pricing_model']} å®šåƒ¹ï¼‰"
            )
            total_line = f"- **ç¸½è¨ˆ: ${cost['total_cost']:.4f}**" if cost['openai']['pricing_available'] else "- **ç¸½è¨ˆ: N/A**"

            st.info(
                "ğŸ’° **è²»ç”¨ä¼°ç®—**\n\n"
                f"- ElevenLabs: {cost['elevenlabs']['minutes']:.2f} åˆ†é˜ = ${cost['elevenlabs']['cost']:.4f}\n"
                f"{openai_line}\n"
                f"{total_line}"
            )

        if result.get('low_match_rate'):
            st.warning("âš ï¸ åŒ¹é…ç‡è¼ƒä½ï¼Œå¯èƒ½æ˜¯å› ç‚ºæ–·å¥æ™‚æ–‡å­—è¢«ä¿®æ”¹äº†ï¼Œæˆ–ç¹ç°¡ä¸ä¸€è‡´ã€‚")

        with st.expander("æŸ¥çœ‹åŸå§‹è½‰éŒ„æ–‡å­—"):
            st.text(result['full_text'])

        if show_debug:
            with st.expander("ğŸ”§ èª¿è©¦ï¼šElevenLabs API åŸå§‹å›æ‡‰"):
                api_response = result.get('raw_api_response', {})
                st.write(f"**API å›æ‡‰ keys:** {list(api_response.keys())}")
                if 'words' in api_response:
                    words = api_response['words']
                    st.write(f"**Words æ•¸é‡:** {len(words)}")
                    if words:
                        st.write(f"**ç¬¬ä¸€å€‹ word çµæ§‹:** {words[0]}")
                st.json(api_response)

        # SRT è¡¨æ ¼é è¦½
        st.subheader("å­—å¹•é è¦½")
        if result.get('srt_data'):
            df = pd.DataFrame([
                {"#": item["index"], "é–‹å§‹": item["start"], "çµæŸ": item["end"], "å­—å¹•": item["text"]}
                for item in result['srt_data']
            ])
            st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            st.text_area("SRT", result['srt_string'], height=300)

        with st.expander("LLM æ–·å¥çµæœ"):
            st.text_area("Segmented", result['segmented_text'], height=200, label_visibility="collapsed")

        with st.expander("SRT åŸå§‹æ–‡å­—"):
            st.text_area("SRT Raw", result['srt_string'], height=200, label_visibility="collapsed")

        # Download Button
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ .srt å­—å¹•æª”",
            data=result['srt_string'],
            file_name=f"{Path(result['filename']).stem}.srt",
            mime="text/plain",
            use_container_width=True
        )

elif not (el_key and oa_key):
    st.info("ğŸ‘ˆ è«‹å…ˆåœ¨å·¦å´è¼¸å…¥ API Keys")
